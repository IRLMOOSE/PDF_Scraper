import time
import requests
import os
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from notion_client import Client
from tenacity import retry, stop_after_attempt, wait_fixed

# Replace this with your Notion token
NOTION_TOKEN = "secret_8Ks2ldrvkPwX9Qwycw5nC6CH0xnV8iOfymc3IMh8vwA"
# Replace this with your Notion database URL
NOTION_DATABASE_URL = "71b3f78794b04682b06c34493bf7dfc9"

client = Client(auth=NOTION_TOKEN)
database_id = NOTION_DATABASE_URL.split("/")[-1]

visited = set()

def log_to_file(message):
    with open('log.txt', 'a') as f:
        f.write(f"{time.ctime()}: {message}\n")

def convert_rich_text_to_plain_text(rich_text):
    # This function converts rich text (HTML) to plain text
    soup = BeautifulSoup(rich_text, "html.parser")
    return soup.get_text()

@retry(stop=stop_after_attempt(3), wait=wait_fixed(30))
def add_to_notion(url, file):
    try:
        log_to_file(f"Adding {file} to Notion...")
        page = client.pages.create(parent={"database_id": database_id}, properties={"Name": {"title": [{"text": {"content": convert_rich_text_to_plain_text(file)}}]}, "URL": {"url": url}})
        log_to_file(f"Added {file} to Notion.")
    except Exception as e:
        log_to_file(f"Failed to add {file} to Notion. Error: {str(e)}")

@retry(stop=stop_after_attempt(3), wait=wait_fixed(30))
def download_file(url):
    try:
        log_to_file(f"Downloading {url}...")
        local_filename = url.split('/')[-1]
        with requests.get(url, stream=True) as r:
            r.raise_for_status()
            with open(local_filename, 'wb') as f:
                for chunk in r.iter_content(chunk_size=8192): 
                    f.write(chunk)
        log_to_file(f"Downloaded {url}.")
        return local_filename
    except Exception as e:
        log_to_file(f"Failed to download {url}. Error: {str(e)}")

@retry(stop=stop_after_attempt(3), wait=wait_fixed(30))
def get_pdfs_from_url(url):
    if url in visited:
        return
    visited.add(url)
    log_to_file(f"Scanning {url} for PDFs...")
    
    soup = BeautifulSoup(requests.get(url).content, "html.parser")
    
    for link in soup.select("a[href$='.pdf']"):
        pdf_url = urljoin(url, link['href'])
        file = download_file(pdf_url)
        add_to_notion(pdf_url, file)
        log_to_file(f"Waiting 30 seconds before next download...")
        time.sleep(30)
    
    for link in soup.find_all('a'):
        page_url = urljoin(url, link.get('href'))
        get_pdfs_from_url(page_url)

def get_pdfs_from_websites(websites):
    for website in websites:
        get_pdfs_from_url(website)

# Replace this list with the websites you want to scan
websites_to_scan = ["", ""]
get_pdfs_from_websites(websites_to_scan)
